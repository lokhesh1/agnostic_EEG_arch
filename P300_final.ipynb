{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03eea36",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f557f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.signal import firwin, filtfilt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import warnings\n",
    "import torch\n",
    "import random\n",
    "from scipy.io import loadmat\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"mne\")\n",
    "# Completely silence MNE-Python output\n",
    "mne.set_log_level('WARNING')  # or 'ERROR' for even less output\n",
    "logging.getLogger('mne').setLevel(logging.WARNING)\n",
    "#mne.set_log_level('debug')\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fbae4a",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055eea4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(path, mode):\n",
    "    chnames = ['F7',\n",
    "                'F3',\n",
    "                'Fz',\n",
    "                'F4',\n",
    "                'F8',\n",
    "                'T7',\n",
    "                'C3',\n",
    "                'Cz',\n",
    "                'C4',\n",
    "                'T8',\n",
    "                'P7',\n",
    "                'P3',\n",
    "                'Pz',\n",
    "                'P4',\n",
    "                'P8',\n",
    "                'O1',\n",
    "                'O2',\n",
    "                'STI 014']\n",
    "    chtypes = ['eeg'] * 17 + ['stim']               \n",
    "\n",
    "    X=loadmat(path)[mode].T\n",
    "    S = X[1:18,:]\n",
    "    stim = (X[18,:] + X[19,:])[None,:]\n",
    "    X = np.concatenate([S, stim])\n",
    "\n",
    "    info = mne.create_info(ch_names=chnames, sfreq=128,\n",
    "                            ch_types=chtypes,\n",
    "                            verbose=False)\n",
    "    raw = mne.io.RawArray(data=X, info=info, verbose=False)\n",
    "\n",
    "    # get rid of the Fz channel (it is the ground)\n",
    "    raw.info['bads'] = ['Fz']\n",
    "    raw.pick_types(eeg=True, stim=True)\n",
    "\n",
    "    sfreq = raw.info['sfreq']  # Hz\n",
    "\n",
    "    # FIR filter params\n",
    "    low_cutoff = 1.0\n",
    "    high_cutoff = 30.0\n",
    "    filter_order = 177\n",
    "    nyquist = 0.5 * sfreq\n",
    "\n",
    "    fir_coeffs = firwin(\n",
    "        numtaps=filter_order,\n",
    "        cutoff=[low_cutoff / nyquist, high_cutoff / nyquist],\n",
    "        pass_zero=False,\n",
    "        window='blackman'\n",
    "    )\n",
    "\n",
    "    # Separate EEG and stim\n",
    "    eeg_picks = mne.pick_types(raw.info, eeg=True)\n",
    "    stim_picks = mne.pick_types(raw.info, stim=True)\n",
    "\n",
    "    eeg_data = raw.get_data(picks=eeg_picks)\n",
    "    stim_data = raw.get_data(picks=stim_picks)  # keep stim intact\n",
    "\n",
    "    # Filter EEG only\n",
    "    filtered_eeg = filtfilt(fir_coeffs, 1.0, eeg_data, axis=1)\n",
    "\n",
    "    # Recombine EEG + stim\n",
    "    filtered_data = np.vstack([filtered_eeg, stim_data])\n",
    "\n",
    "    # Recreate Raw object\n",
    "    new_raw = mne.io.RawArray(filtered_data, raw.info.copy())\n",
    "    new_raw.set_annotations(raw.annotations)\n",
    "\n",
    "    # Events and epochs\n",
    "    events = mne.find_events(new_raw, shortest_event=1, verbose=False)\n",
    "    events_id = {'NonTarget': 1, 'Target': 2}\n",
    "\n",
    "    epochs = mne.Epochs(\n",
    "        new_raw,\n",
    "        events=events,\n",
    "        tmin=-0.2,\n",
    "        tmax=0.8,\n",
    "        event_id=events_id,\n",
    "        baseline=None,\n",
    "        preload=True\n",
    "    )\n",
    "\n",
    "    labels = epochs.events[:, 2]\n",
    "    data = epochs.get_data()\n",
    "    labels = labels - 1\n",
    "    return {\n",
    "        'epochs': data,   \n",
    "        'labels': labels\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c3a9d1",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "def gauss_smooth(inputs, device, smooth_kernel_std=2, smooth_kernel_size=100,  padding='same'):\n",
    "\n",
    "    #print(inputs.shape)\n",
    "    inputs = inputs.transpose(0, 2, 1)\n",
    "    # Get Gaussian kernel\n",
    "    inp = np.zeros(smooth_kernel_size, dtype=np.float32)\n",
    "    inp[smooth_kernel_size // 2] = 1\n",
    "    gaussKernel = gaussian_filter1d(inp, smooth_kernel_std)\n",
    "    validIdx = np.argwhere(gaussKernel > 0.01)\n",
    "    gaussKernel = gaussKernel[validIdx]\n",
    "    gaussKernel = np.squeeze(gaussKernel / np.sum(gaussKernel))\n",
    "\n",
    "    # Convert to tensor\n",
    "    gaussKernel = torch.tensor(gaussKernel, dtype=torch.float32, device=device)\n",
    "    gaussKernel = gaussKernel.view(1, 1, -1)  # [1, 1, kernel_size]\n",
    "\n",
    "    # Prepare convolution\n",
    "    B, T, C = inputs.shape\n",
    "    inputs = inputs.transpose(0, 2, 1)  # [B, C, T]\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32, device=device)\n",
    "    \n",
    "    gaussKernel = gaussKernel.repeat(C, 1, 1)  # [C, 1, kernel_size]\n",
    "\n",
    "    # Perform convolution\n",
    "    smoothed = F.conv1d(inputs, gaussKernel, padding=padding, groups=C)\n",
    "    return smoothed  # [B, T, C]\n",
    "\n",
    "from scipy.signal import filtfilt, butter\n",
    "\n",
    "def bandpass(data, low=1, high=40, fs=128, order=4):\n",
    "    nyq = fs / 2\n",
    "    b, a = butter(order, [low/nyq, high/nyq], btype='band')\n",
    "    \n",
    "    # Apply on axis=2 (time axis)\n",
    "    return filtfilt(b, a, data, axis=2)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class BCI4_2a_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_dir, mode, subjects=[1], transform=None, target_transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.subjects = subjects\n",
    "        self.data, self.labels = self._load_data()\n",
    "\n",
    "        self.labels = self.labels \n",
    "        # In your dataset __init__:\n",
    "        print(f\"Class distribution: {np.bincount(self.labels)}\")\n",
    "        print(f\"Class ratio: {np.bincount(self.labels)[1] / len(self.labels):.2%}\")\n",
    "\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        label = []\n",
    "\n",
    "        for i in self.subjects:\n",
    "            path = f\"{self.data_dir}/subject_{i:02d}/{self.mode}.mat\"\n",
    "            dat = process(path, self.mode)\n",
    "            data.append(dat['epochs'])\n",
    "            label.extend(dat['labels'])\n",
    "\n",
    "        return np.vstack(data), label\n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]\n",
    "        y = self.labels[idx]\n",
    "        \n",
    "        x = torch.from_numpy(x).float()  # shape: (1, timepoints, channels)\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        if self.target_transform:\n",
    "            y = self.target_transform(y)\n",
    "            \n",
    "        return x, y\n",
    "\n",
    "def get_data_loaders(data_dir,all_subjects=[1], test_subject=[1,2], batch_size=32, val_split=0.2, random_state=42):\n",
    "    # Get all subjects except the test subject for training/validation\n",
    "    \n",
    "    #train_val_subjects = [s for s in all_subjects if s not in test_subject]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_val_dataset = BCI4_2a_Dataset(data_dir,'training', subjects=all_subjects)\n",
    "    test_dataset = BCI4_2a_Dataset(data_dir,'online', subjects=all_subjects)\n",
    "    single_trial_shape = test_dataset.data[0].shape\n",
    "    labels = np.array(train_val_dataset.labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=np.unique(labels),\n",
    "                                     y=labels)\n",
    "\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    print(f\"Train+Val dataset size: {len(train_val_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "    # Split train_val into train and validation\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        range(len(train_val_dataset)),\n",
    "        test_size=val_split,\n",
    "        random_state=random_state,\n",
    "        stratify=train_val_dataset.labels\n",
    "    )\n",
    "    \n",
    "    train_dataset = torch.utils.data.Subset(train_val_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(train_val_dataset, val_idx)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Train batches: {len(train_loader)}\")\n",
    "    print(f\"Val batches: {len(val_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    print(f\"Single Trial Shape: {single_trial_shape}\")\n",
    "    return train_loader, val_loader, test_loader, single_trial_shape, class_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a4378",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "135ecd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=4, dropout=0.1):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),  # Added dropout\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channel_avg = torch.mean(x, dim=2, keepdim=True)  \n",
    "        channel_max, _ = torch.max(x, dim=2, keepdim=True) \n",
    "        combined = channel_avg + channel_max  \n",
    "        combined = combined.squeeze(2)\n",
    "        attention = self.mlp(combined)\n",
    "        attention = torch.sigmoid(attention).unsqueeze(2)\n",
    "        attended_x = x * attention\n",
    "        return attended_x, attention.squeeze(2)  # Return proper attention scores\n",
    "\n",
    "\n",
    "class LearnableSTFT(nn.Module):\n",
    "    def __init__(self, window_size, hop_size, learnable_window=True, dropout=0.05):\n",
    "        super(LearnableSTFT, self).__init__()\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.hop_size = hop_size\n",
    "        self.dft_size = window_size\n",
    "        self.learnable_window = learnable_window\n",
    "        \n",
    "        # Initialize with Hamming window\n",
    "        initial_window = 0.54 - 0.46 * torch.cos(\n",
    "            2 * math.pi * torch.arange(window_size, dtype=torch.float32) / (window_size - 1)\n",
    "        )\n",
    "        \n",
    "        if learnable_window:\n",
    "            self.window = nn.Parameter(initial_window)\n",
    "        else:\n",
    "            # Fixed window reduces overfitting\n",
    "            self.register_buffer('window', initial_window)\n",
    "        \n",
    "        # Batch normalization after STFT\n",
    "        self.batch_norm = nn.BatchNorm2d(1)\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        \n",
    "        dft_matrix = self._create_dft_matrix(self.dft_size, self.window_size)\n",
    "        self.register_buffer('dft_matrix', dft_matrix)\n",
    "\n",
    "    def _create_dft_matrix(self, dft_size, window_size):\n",
    "        k = torch.arange(dft_size).unsqueeze(1)\n",
    "        n = torch.arange(window_size)\n",
    "        angle = -2 * math.pi * k * n / dft_size\n",
    "        dft_matrix = torch.complex(torch.cos(angle), torch.sin(angle))\n",
    "        return dft_matrix\n",
    "\n",
    "    def forward(self, signal):\n",
    "        if signal.dim() == 1:\n",
    "            signal = signal.unsqueeze(0).unsqueeze(0)\n",
    "        elif signal.dim() == 2:\n",
    "            signal = signal.unsqueeze(1)\n",
    "            \n",
    "        batch_size, num_channels, num_samples = signal.shape\n",
    "        signal_reshaped = signal.reshape(batch_size * num_channels, num_samples)\n",
    "        \n",
    "        frames = signal_reshaped.unfold(dimension=1, size=self.window_size, step=self.hop_size)\n",
    "        num_frames_unfolded = frames.shape[1]\n",
    "        expected_num_frames = int(math.ceil((num_samples - self.window_size) / self.hop_size)) + 1\n",
    "        \n",
    "        if num_frames_unfolded < expected_num_frames:\n",
    "            padding_amount = (expected_num_frames - 1) * self.hop_size + self.window_size - num_samples\n",
    "            padded_signal = torch.nn.functional.pad(signal_reshaped, (0, padding_amount))\n",
    "            frames = padded_signal.unfold(1, self.window_size, self.hop_size)\n",
    "        \n",
    "        windowed_frames = frames * self.window\n",
    "        BC, F, W = windowed_frames.shape\n",
    "        windowed_frames_reshaped = windowed_frames.reshape(BC * F, W)\n",
    "        \n",
    "        windowed_frames_complex = windowed_frames_reshaped.to(self.dft_matrix.dtype)\n",
    "        stft_result_reshaped = self.dft_matrix @ windowed_frames_complex.T\n",
    "        stft_result = stft_result_reshaped.T.reshape(batch_size, num_channels, F, self.dft_size)\n",
    "        \n",
    "        # Apply magnitude and normalization\n",
    "        stft_magnitude = torch.abs(stft_result)\n",
    "        \n",
    "        # Reshape for batch norm: (B*C, 1, F, freq_bins)\n",
    "        stft_normalized = stft_magnitude.reshape(batch_size * num_channels, 1, F, self.dft_size)\n",
    "        stft_normalized = self.batch_norm(stft_normalized)\n",
    "        stft_normalized = self.dropout(stft_normalized)\n",
    "        stft_normalized = stft_normalized.reshape(batch_size, num_channels, F, self.dft_size)\n",
    "        \n",
    "        return stft_normalized\n",
    "\n",
    "\n",
    "class Attention4D(nn.Module):\n",
    "    def __init__(self, in_channels, time_frames, freq_bins, d_model=128, n_heads=4, \n",
    "                 d_ff=256, dropout=0.2, num_layers=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.time_frames = time_frames\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        in_features = in_channels * freq_bins\n",
    "        \n",
    "        # Project input to d_model with layer norm\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, d_model),\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Learnable positional encoding with smaller init\n",
    "        self.positional_encoding = nn.Parameter(\n",
    "            torch.randn(1, time_frames, d_model) * 0.02\n",
    "        )\n",
    "        \n",
    "        # Stack multiple transformer layers for better representation\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Reshape: (B, C, T, F) -> (B, T, C*F)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        x = x.reshape(batch_size, self.time_frames, -1)\n",
    "        \n",
    "        # Project and add positional encoding\n",
    "        x = self.projection(x)\n",
    "        x = x + self.positional_encoding\n",
    "        \n",
    "        # Store attention weights from all layers\n",
    "        all_attention_weights = []\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer_layers:\n",
    "            x, attn_weights = layer(x)\n",
    "            all_attention_weights.append(attn_weights)\n",
    "        \n",
    "        x = self.final_norm(x)\n",
    "        \n",
    "        return x, all_attention_weights\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            dropout=dropout, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),  # GELU often works better than ReLU\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture (more stable)\n",
    "        x_norm = self.layernorm1(x)\n",
    "        attn_output, attn_weights = self.attention(x_norm, x_norm, x_norm, \n",
    "                                                    need_weights=True, \n",
    "                                                    average_attn_weights=True)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        \n",
    "        x_norm = self.layernorm2(x)\n",
    "        ff_output = self.feed_forward(x_norm)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        # Add intermediate layer for better capacity\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, in_features // 2),\n",
    "            nn.LayerNorm(in_features // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, d_model, T)\n",
    "        x = self.pooling(x).squeeze(2)  # (B, d_model)\n",
    "        output = self.classifier(x)  # (B, num_classes)\n",
    "        return output\n",
    "\n",
    "\n",
    "class EEGClassifier(nn.Module):\n",
    "    def __init__(self, num_samples, num_classes=4, window_percent=0.25, \n",
    "                 overlap_percent=0.10, d_model=128, n_heads=8, \n",
    "                 transformer_layers=1, dropout=0.2, learnable_window=True):\n",
    "\n",
    "        super(EEGClassifier, self).__init__()\n",
    "        \n",
    "        window_size = int(num_samples * window_percent)\n",
    "        hop_size = int(num_samples * overlap_percent)\n",
    "        \n",
    "        self.window_size = max(window_size, 40)\n",
    "        self.hop_size = max(hop_size, 20)\n",
    "        \n",
    "        self.channel_attn = ChannelAttention(num_channels=17, dropout=dropout)\n",
    "        \n",
    "        self.learnable_stft = LearnableSTFT(\n",
    "            window_size=self.window_size,\n",
    "            hop_size=self.hop_size,\n",
    "            learnable_window=learnable_window,\n",
    "            dropout=dropout * 0.5\n",
    "        )\n",
    "        \n",
    "        time_frames = int(math.ceil((num_samples - self.window_size) / self.hop_size)) + 1\n",
    "        \n",
    "        self.attention_module = Attention4D(\n",
    "            in_channels=17,\n",
    "            time_frames=time_frames,\n",
    "            freq_bins=self.window_size,\n",
    "            d_model=d_model,\n",
    "            n_heads=n_heads,\n",
    "            num_layers=transformer_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = Classifier(\n",
    "            in_features=d_model, \n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "\n",
    "        # Channel attention\n",
    "        x, channel_attn_scores = self.channel_attn(x)\n",
    "        \n",
    "        # STFT\n",
    "        x = self.learnable_stft(x)\n",
    "        \n",
    "        # Transformer attention\n",
    "        x, transformer_attn_weights = self.attention_module(x)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        if return_attention:\n",
    "            attention_dict = {\n",
    "                'channel_attention': channel_attn_scores,  # (B, 22)\n",
    "                'transformer_attention': transformer_attn_weights,  # List of (B, T, T) for each layer\n",
    "            }\n",
    "            return logits, attention_dict\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class AttentionRegularizedLoss(nn.Module):\n",
    "    def __init__(self, num_classes, alpha=0.01, beta=0.01):\n",
    "        super().__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha  # Channel attention regularization weight\n",
    "        self.beta = beta    # Transformer attention regularization weight\n",
    "        \n",
    "    def forward(self, logits, targets, attention_dict):\n",
    "        ce = self.ce_loss(logits, targets)\n",
    "        channel_attn = attention_dict['channel_attention']  # (B, C)\n",
    "        channel_attn_norm = channel_attn + 1e-8\n",
    "        channel_attn_norm = channel_attn_norm / channel_attn_norm.sum(dim=1, keepdim=True)\n",
    "        channel_entropy = -(channel_attn_norm * torch.log(channel_attn_norm)).sum(dim=1).mean()\n",
    "        channel_reg = -self.alpha * channel_entropy  # Negative because we want to maximize entropy\n",
    "        transformer_attn = attention_dict['transformer_attention'][-1]  # (B, T, T)\n",
    "        attn_norm = transformer_attn + 1e-8\n",
    "        attn_norm = attn_norm / attn_norm.sum(dim=-1, keepdim=True)\n",
    "        attn_entropy = -(attn_norm * torch.log(attn_norm)).sum(dim=-1).mean()\n",
    "        transformer_reg = -self.beta * attn_entropy  # Maximize entropy\n",
    "\n",
    "        total_loss = ce + channel_reg + transformer_reg\n",
    "        \n",
    "        return total_loss, {\n",
    "            'ce_loss': ce.item(),\n",
    "            'channel_reg': channel_reg.item(),\n",
    "            'transformer_reg': transformer_reg.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f850fe",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "484720f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Model initialization\n",
    "#     model = EEGClassifier(\n",
    "#         num_samples=1000,\n",
    "#         num_classes=4,\n",
    "#         d_model=64,  # Reduced from 128\n",
    "#         n_heads=4,   # Reduced from 8\n",
    "#         transformer_layers=2,\n",
    "#         dropout=0.3,\n",
    "#         learnable_window=False  # Start with fixed window\n",
    "#     )\n",
    "    \n",
    "#     # Loss function\n",
    "#     criterion = AttentionRegularizedLoss(num_classes=4, alpha=0.01, beta=0.01)\n",
    "    \n",
    "#     # Dummy data\n",
    "#     x = torch.randn(4, 22, 1000)  # (batch, channels, time)\n",
    "#     y = torch.randint(0, 4, (4,))\n",
    "    \n",
    "#     # Forward pass with attention\n",
    "#     logits, attention_dict = model(x, return_attention=True)\n",
    "    \n",
    "#     # Calculate loss\n",
    "#     loss, loss_dict = criterion(logits, y, attention_dict)\n",
    "    \n",
    "#     print(f\"Logits shape: {logits.shape}\")\n",
    "#     print(f\"Channel attention shape: {attention_dict['channel_attention'].shape}\")\n",
    "#     print(f\"Num transformer layers: {len(attention_dict['transformer_attention'])}\")\n",
    "#     print(f\"Transformer attention shape: {attention_dict['transformer_attention'][0].shape}\")\n",
    "#     print(f\"Total loss: {loss.item():.4f}\")\n",
    "#     print(f\"Loss components: {loss_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f023b",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c483a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 10:30:03.299073: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-29 10:30:03.359604: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-29 10:30:03.359650: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-29 10:30:03.359690: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-29 10:30:03.371680: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-29 10:30:04.530009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import json\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "class EEGTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader=None,class_weights=None, config=None):\n",
    "\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.class_weights = class_weights.to(self.device)\n",
    "        # Default configuration\n",
    "        self.config = {\n",
    "            'lr': 1e-3,\n",
    "            'weight_decay': 1e-4,\n",
    "            'patience': 10,\n",
    "            'min_lr': 1e-6,\n",
    "            'epochs': 100,\n",
    "            'save_dir': 'experiments',\n",
    "            'experiment_name': f'exp_{time.strftime(\"%Y%m%d-%H%M%S\")}',\n",
    "            'save_attention_maps': True,\n",
    "            'attention_map_freq': 5\n",
    "        }\n",
    "        \n",
    "        # Update with user config if provided\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "            \n",
    "        # Create experiment directory\n",
    "        self.exp_dir = os.path.join(self.config['save_dir'], self.config['experiment_name'])\n",
    "        os.makedirs(self.exp_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_components()\n",
    "        \n",
    "        # Save config\n",
    "        self._save_config()\n",
    "        \n",
    "    def _init_components(self):\n",
    "        \"\"\"Initialize training components\"\"\"\n",
    "        # Loss function (CrossEntropy + KLDiv for attention regularization)\n",
    "        self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=self.config['lr'],\n",
    "            weight_decay=self.config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='max',\n",
    "            factor=0.5,\n",
    "            patience=self.config['patience']//2,\n",
    "            min_lr=self.config['min_lr'],\n",
    "            #verbose=True\n",
    "        )\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.early_stop_counter = 0\n",
    "        \n",
    "        # Tensorboard writer\n",
    "        self.writer = SummaryWriter(log_dir=self.exp_dir)\n",
    "        \n",
    "        # Attention maps directory\n",
    "        if self.config['save_attention_maps']:\n",
    "            self.attention_dir = os.path.join(self.exp_dir, 'attention_maps')\n",
    "            os.makedirs(self.attention_dir, exist_ok=True)\n",
    "    \n",
    "    def _save_config(self):\n",
    "        config_path = os.path.join(self.exp_dir, 'config.json')\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=4)\n",
    "    \n",
    "    def _compute_metrics(self, outputs, labels):\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        return accuracy\n",
    "    \n",
    "    def _log_metrics(self, phase, metrics, epoch):\n",
    "        loss = metrics['loss']\n",
    "        acc = metrics['accuracy']\n",
    "        pre = metrics['precision']\n",
    "        rec = metrics['recall']\n",
    "        f1 = metrics['f1']\n",
    "        \n",
    "        # Console logging\n",
    "        print(f\"{phase.capitalize()} - Epoch: {epoch+1} | \"\n",
    "              f\"Loss: {loss:.4f} | Acc: {acc:.2%} | Precision: {pre:.2%} | recall: {rec:.2%} | f1: {f1:.2%}\")\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        self.writer.add_scalar(f'Loss/{phase}', loss, epoch)\n",
    "        self.writer.add_scalar(f'Accuracy/{phase}', acc, epoch)\n",
    "        \n",
    "        \n",
    "        # Log learning rate\n",
    "        if phase == 'train':\n",
    "            lr = self.optimizer.param_groups[0]['lr']\n",
    "            self.writer.add_scalar('LR', lr, epoch)\n",
    "    \n",
    "    def _save_checkpoint(self, epoch, is_best=False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'scheduler': self.scheduler.state_dict(),\n",
    "            'best_val_acc': self.best_val_acc\n",
    "        }\n",
    "        \n",
    "        # Save regular checkpoint\n",
    "        checkpoint_path = os.path.join(self.exp_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "        torch.save(state, checkpoint_path)\n",
    "        \n",
    "        # Save best model\n",
    "        if is_best:\n",
    "            best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "            torch.save(state, best_path)\n",
    "    \n",
    "\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Store all predictions and labels for epoch-wise metrics\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for inputs, labels in self.train_loader:\n",
    "            inputs = inputs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            # Zero gradients\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, attention_dict = self.model(inputs, return_attention=True)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # ----- accumulate loss -----\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "            # ----- accumulate preds + labels for metrics -----\n",
    "            preds = torch.argmax(outputs, dim=1)          # [B]\n",
    "            all_preds.append(preds.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "\n",
    "        # ---- end of epoch: stack everything and compute metrics ----\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "        epoch_loss = running_loss / total_samples\n",
    "\n",
    "        # accuracy\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # precision, recall, f1\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels,\n",
    "            all_preds,\n",
    "            average='binary',      # change to 'macro' / 'binary' if needed\n",
    "            zero_division=0          # avoid NaN if a class is missing in preds\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'loss': epoch_loss,\n",
    "            'accuracy': epoch_acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "    def validate_epoch(self, epoch):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.val_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "     \n",
    "                # Forward pass\n",
    "                #outputs = self.model(inputs)\n",
    "                outputs, attention_dict = self.model(inputs, return_attention=True)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                #loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                batch_size = inputs.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # ----- accumulate preds + labels for metrics -----\n",
    "                preds = torch.argmax(outputs, dim=1)          # [B]\n",
    "                all_preds.append(preds.detach().cpu())\n",
    "                all_labels.append(labels.detach().cpu())\n",
    "\n",
    "            # ---- end of epoch: stack everything and compute metrics ----\n",
    "            all_preds = torch.cat(all_preds).numpy()\n",
    "            all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "            epoch_loss = running_loss / total_samples\n",
    "\n",
    "            # accuracy\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            # precision, recall, f1\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                average='binary',      # change to 'macro' / 'binary' if needed\n",
    "                zero_division=0          # avoid NaN if a class is missing in preds\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'loss': epoch_loss,\n",
    "                'accuracy': epoch_acc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "            }\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            # Train and validate\n",
    "            train_metrics = self.train_epoch(epoch)\n",
    "            val_metrics = self.validate_epoch(epoch)\n",
    "            \n",
    "            # Log metrics\n",
    "            self._log_metrics('train', train_metrics, epoch)\n",
    "            self._log_metrics('val', val_metrics, epoch)\n",
    "            \n",
    "            # Save attention maps periodically\n",
    "            \n",
    "            # Step scheduler\n",
    "            self.scheduler.step(val_metrics['accuracy'])\n",
    "            \n",
    "            # Check for best model\n",
    "            if val_metrics['accuracy'] > self.best_val_acc:\n",
    "                self.best_val_acc = val_metrics['accuracy']\n",
    "                self._save_checkpoint(epoch, is_best=True)\n",
    "                self.early_stop_counter = 0\n",
    "            else:\n",
    "                self.early_stop_counter += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if epoch % 10 == 0:\n",
    "                self._save_checkpoint(epoch)\n",
    "            \n",
    "            # Early stopping\n",
    "            # if self.early_stop_counter >= self.config['patience']:\n",
    "            #     print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            #     break\n",
    "        \n",
    "        # Training complete\n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"Training completed in {training_time//60:.0f}m {training_time%60:.0f}s\")\n",
    "        print(f\"Best validation accuracy: {self.best_val_acc:.2%}\")\n",
    "        \n",
    "        # Test if test loader provided\n",
    "        if self.test_loader:\n",
    "            test_acc = self.test()\n",
    "            print(f\"Test accuracy: {test_acc:.2%}\")\n",
    "        \n",
    "        # Close tensorboard writer\n",
    "        self.writer.close()\n",
    "        \n",
    "        return self.best_val_acc\n",
    "    \n",
    "    def test(self):\n",
    "        \"\"\"Evaluate on test set\"\"\"\n",
    "        self.model.eval()\n",
    "        running_acc = 0.0\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Load best model\n",
    "        best_path = os.path.join(self.exp_dir, 'best_model.pth')\n",
    "        if os.path.exists(best_path):\n",
    "            checkpoint = torch.load(best_path)\n",
    "            self.model.load_state_dict(checkpoint['state_dict'])\n",
    "            print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "        \n",
    "        all_preds=[]\n",
    "        all_labels=[]\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in self.test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                batch_size = inputs.size(0)\n",
    "                total_samples += batch_size\n",
    "\n",
    "                # ----- accumulate preds + labels for metrics -----\n",
    "                preds = torch.argmax(outputs, dim=1)          # [B]\n",
    "                all_preds.append(preds.detach().cpu())\n",
    "                all_labels.append(labels.detach().cpu())\n",
    "\n",
    "            # ---- end of epoch: stack everything and compute metrics ----\n",
    "            all_preds = torch.cat(all_preds).numpy()\n",
    "            all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "            # precision, recall, f1\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                all_labels,\n",
    "                all_preds,\n",
    "                average='binary',      # change to 'macro' / 'binary' if needed\n",
    "                zero_division=0          # avoid NaN if a class is missing in preds\n",
    "            )\n",
    "        \n",
    "        test_acc = epoch_acc\n",
    "        self.writer.add_scalar('Accuracy/test', test_acc)\n",
    "        print(\"acc: \",test_acc)\n",
    "        print(\"precision:\", precision)\n",
    "        print(\"recall:\", recall)\n",
    "        print(\"F1 :\", f1)\n",
    "        return epoch_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef686541",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf259697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: [15955  3189]\n",
      "Class ratio: 16.66%\n",
      "Class distribution: [11399  2279]\n",
      "Class ratio: 16.66%\n",
      "Train+Val dataset size: 19144\n",
      "Test dataset size: 13678\n",
      "Train batches: 479\n",
      "Val batches: 120\n",
      "Test batches: 428\n",
      "Single Trial Shape: (17, 129)\n",
      "Train - Epoch: 1 | Loss: 0.6965 | Acc: 61.55% | Precision: 17.33% | recall: 34.69% | f1: 23.11%\n",
      "Val - Epoch: 1 | Loss: 0.6923 | Acc: 44.24% | Precision: 18.64% | recall: 69.75% | f1: 29.42%\n",
      "Train - Epoch: 2 | Loss: 0.6948 | Acc: 65.63% | Precision: 16.79% | recall: 26.89% | f1: 20.68%\n",
      "Val - Epoch: 2 | Loss: 0.6931 | Acc: 17.52% | Precision: 16.70% | recall: 99.06% | f1: 28.58%\n",
      "Train - Epoch: 3 | Loss: 0.6924 | Acc: 65.34% | Precision: 18.08% | recall: 30.62% | f1: 22.74%\n",
      "Val - Epoch: 3 | Loss: 0.6904 | Acc: 34.55% | Precision: 18.57% | recall: 86.52% | f1: 30.58%\n",
      "Train - Epoch: 4 | Loss: 0.6909 | Acc: 64.79% | Precision: 19.00% | recall: 34.14% | f1: 24.41%\n",
      "Val - Epoch: 4 | Loss: 0.6892 | Acc: 30.19% | Precision: 18.31% | recall: 92.16% | f1: 30.55%\n",
      "Train - Epoch: 5 | Loss: 0.6847 | Acc: 58.64% | Precision: 19.85% | recall: 48.84% | f1: 28.23%\n",
      "Val - Epoch: 5 | Loss: 0.6655 | Acc: 67.72% | Precision: 25.37% | recall: 48.28% | f1: 33.26%\n",
      "Train - Epoch: 6 | Loss: 0.5237 | Acc: 68.97% | Precision: 31.39% | recall: 72.76% | f1: 43.86%\n",
      "Val - Epoch: 6 | Loss: 0.0544 | Acc: 98.09% | Precision: 89.96% | recall: 99.69% | f1: 94.57%\n",
      "Train - Epoch: 7 | Loss: 0.1540 | Acc: 92.74% | Precision: 71.69% | recall: 93.22% | f1: 81.05%\n",
      "Val - Epoch: 7 | Loss: 0.0151 | Acc: 99.40% | Precision: 96.52% | recall: 100.00% | f1: 98.23%\n",
      "Train - Epoch: 8 | Loss: 0.1032 | Acc: 95.79% | Precision: 84.56% | recall: 91.45% | f1: 87.87%\n",
      "Val - Epoch: 8 | Loss: 0.0663 | Acc: 97.65% | Precision: 87.64% | recall: 100.00% | f1: 93.41%\n",
      "Train - Epoch: 9 | Loss: 0.0968 | Acc: 95.20% | Precision: 80.55% | recall: 93.85% | f1: 86.69%\n",
      "Val - Epoch: 9 | Loss: 0.0017 | Acc: 99.97% | Precision: 99.84% | recall: 100.00% | f1: 99.92%\n",
      "Train - Epoch: 10 | Loss: 0.0819 | Acc: 96.72% | Precision: 88.76% | recall: 91.92% | f1: 90.31%\n",
      "Val - Epoch: 10 | Loss: 0.0012 | Acc: 100.00% | Precision: 100.00% | recall: 100.00% | f1: 100.00%\n",
      "Training completed in 0m 40s\n",
      "Best validation accuracy: 100.00%\n",
      "Loaded best model from epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_220804/1303434921.py:307: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(best_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:  0.9998537797923673\n",
      "precision: 0.9991231915826392\n",
      "recall: 1.0\n",
      "F1 : 0.999561403508772\n",
      "Test accuracy: 99.99%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example configuration\n",
    "    config = {\n",
    "        'lr': 3e-4,\n",
    "        'weight_decay': 1e-5,\n",
    "        'patience': 15,\n",
    "        'epochs': 10,\n",
    "        'experiment_name': 'eeg_attention_experiment',\n",
    "        'save_attention_maps': True\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_loader, val_loader, test_loader, trail_shape, class_weights = get_data_loaders(data_dir=\"/teamspace/studios/shared-amethyst-w577/data\", all_subjects = range(1,26))\n",
    "    model = EEGClassifier(num_samples = trail_shape[-1], num_classes=2)\n",
    "    # Create trainer\n",
    "    trainer = EEGTrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        class_weights = class_weights,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    best_val_acc = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c10d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
